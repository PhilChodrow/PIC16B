{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5748ac",
   "metadata": {},
   "source": [
    "# Linear Algebra I\n",
    "\n",
    "Linear algebra is a core topic in modern applied mathematics. Essentially every important method in statistics, data science, and machine learning is built on linear algebra. Indeed, deep neural networks, which we will discuss shortly, are built on a foundation of matrix multiplication coupled with simple nonlinear functions. \n",
    "\n",
    "<figure class=\"image\" style=\"width:30%\">\n",
    "  <img src=\"https://imgs.xkcd.com/comics/machine_learning_2x.png\" alt=\"\">\n",
    "  <figcaption><i></i></figcaption>\n",
    "</figure>\n",
    "\n",
    "In this lecture, we'll see how to perform several important operations in linear algebra using our good friend, Numpy. These operations include: \n",
    "\n",
    "- Matrix multiplication. \n",
    "- Exact and approximate solutions to linear systems. \n",
    "- Singular value and eigenvalue-eigenvector decompositions. \n",
    "\n",
    "Along the way, we'll show several examples from statistics and applied mathematics, including simulation of partial differential equations; least-squares regression; and image compression. \n",
    "\n",
    "This is probably the lecture in which things will get \"the most mathematical.\" Comfort with concepts from Math 33A or equivalent will be helpful. If you're not familiar with these concepts, that's ok -- feel free to ask questions. We'll all get through this just fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051120b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no fancy packages this time! just our good friends numpy and matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b3ff1",
   "metadata": {},
   "source": [
    "## Basic Matrix Operations\n",
    "\n",
    "A *matrix* is a two-dimensional array of numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random matrix data to play with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedfe61e",
   "metadata": {},
   "source": [
    "Matrices admit several standard operations, including: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74293632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# application of transposition: a \"symmetrized\" version of A\n",
    "# symmetric matrices satisfy A = A.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb0d40",
   "metadata": {},
   "source": [
    "Inversion is an especially important matrix operation. The inverse $\\mathbf{A}^{-1}$ of a square matrix $\\mathbf{A}$ satisfies $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix. We'll see how to multiply matrices and check this in a sec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse of A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81e0ea",
   "metadata": {},
   "source": [
    "## Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be3aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82692921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix-vector product\n",
    "\n",
    "\n",
    "# modern, convenient syntax -- same effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e160aba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec830806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random matrix\n",
    "\n",
    "\n",
    "# matrix-matrix product (same as A.dot(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a17aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking our inverse from earlier\n",
    "# observe--finite precision arithmetic! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # looks like the identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identity matrix\n",
    "\n",
    "\n",
    "# check the result to within machine precision, entrywise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afaf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d46fe",
   "metadata": {},
   "source": [
    "## Application: Simulating Heat Diffusion\n",
    "\n",
    "Matrix multiplication provides a simple way to simulate 1-dimensional partial differential equations in discrete time. For example, the 1-d heat equation reads\n",
    "\n",
    "$$\\frac{\\partial f(x, t)}{\\partial t} = \\frac{\\partial^2 f}{\\partial x^2 }\\;.$$\n",
    "\n",
    "In a discrete approximation, we can write this as \n",
    "\n",
    "$$f(x_i, t + 1) - f(x_i, t) \\approx \\epsilon\\left[f(x_{i+1}, t) - 2f(x_i, t) + f(x_{i-1}, t)\\right]\\;,$$\n",
    "\n",
    "where $\\epsilon$ is a small positive number that controls the timescale of the approximation. \n",
    "\n",
    "We can write the righthand side of this equation as a matrix-vector product:\n",
    "\n",
    "- Let $\\mathbf{v}(t)$ be the values of $f(\\mathbf{x}, t)$ -- that is, $v_i = f(x_i)$. \n",
    "- Let $\\mathbf{A}$ be the *transition operator*: \n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\left[\\begin{matrix}\n",
    "    -2 &  1 & 0 & \\cdots& 0& 0 & 0\\\\\n",
    "     1 & -2 & 1 & \\cdots & 0& 0 &  0\\\\\n",
    "     0 & 1 & -2 & \\cdots & 0& 0 &  0\\\\\n",
    "     \\vdots & \\vdots &\\vdots & \\ddots &  \\vdots & \\vdots & \\vdots \\\\ \n",
    "     0  & 0  & 0  & \\cdots   & -2     & 1 & 0\\\\\n",
    "     0  & 0  & 0  & \\cdots   & 1     & -2 & 1\\\\\n",
    "     0 & 0  & 0  & \\cdots & 0     & 1     & -2 \\\\\n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "This transition operator has the property that $[\\mathbf{A}\\mathbf{v}]_i$ is equal to the righthand side of the discrete approximation, for $i = 2,\\ldots,n-1$. That is, we can write \n",
    "\n",
    "$$\n",
    "\\mathbf{v}(t+1) = \\mathbf{v}(t) + \\epsilon \\mathbf{A}\\mathbf{v}(t) = (\\mathbf{I} + \\epsilon\\mathbf{A})\\mathbf{v}(t)\n",
    "$$\n",
    "\n",
    "Note that there are issues at the boundary (i.e. where $i = 1$ or $i = n$), and further modeling decisions must be made in order to handle these. In the transition operator above, we are effectively allowing heat to escape at the boundaries. \n",
    "\n",
    "To simulate heat diffusion in Python, we can just build this transition operator as a matrix (`numpy` array) and then iterate this update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be489814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of simulation\n",
    "n = 201\n",
    "\n",
    "# Construct A using the handy np.diag() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a563e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct initial condition: 1 unit of heat at midpoint. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate diffusion and plot, time intervals of 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f83a19",
   "metadata": {},
   "source": [
    "We observe the bell-shaped curve (Gaussian distribution) characteristic of 1d diffusion, just as we'd expect. Note that once we constructed the discretized approximation, we were able to perform the simulation in Python purely via linear algebra! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b039ac6",
   "metadata": {},
   "source": [
    "## Solving Linear Equations\n",
    "\n",
    "One of the most fundamental tasks in applied mathematics is solving linear systems of the form \n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\;,$$\n",
    "\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$, $\\mathbf{x} \\in \\mathbb{R}^{m}$, and $\\mathbf{b} \\in \\mathbb{R}^{n}$. This equation represents a set of linear relationships between variables, a single one of which looks like this: \n",
    "\n",
    "$$\n",
    "a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{im}x_m = b_i\\;.\n",
    "$$\n",
    "\n",
    "Collectively, the equations in a linear system define a \"flat space\" called a *affine subspace* of $\\mathbb{R}^m$.  \n",
    "\n",
    "> 1. When $\\mathbf{A}$ is square and of full rank (determinant nonzero), this equation has a unique solution. \n",
    "> 2. When $\\mathbf{A}$ is not square or not of full rank, then this equation may have either 0 or infinitely many solutions. \n",
    "\n",
    "In Case 1 (\"the good case\"), we can use a simple built-in `numpy` method: `np.linalg.solve`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve A@x = b for x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079caf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual approach (not as efficient)\n",
    "# compute the inverse explicitly and \n",
    "# premultiply by it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f803d",
   "metadata": {},
   "source": [
    "In Case 2 (\"the bad case\"), in which the matrix is either not of full rank or not square, we need to resort to subtler means. Suppose that the matrix $\\mathbf{A}$ has more rows than columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fc1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b78edbe",
   "metadata": {},
   "source": [
    "In this case, there usually are no exact solutions to the equation $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$. If we try the method from before, `numpy` will complain at us: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa6e65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8389002b",
   "metadata": {},
   "source": [
    "One of the most common ways to approach this kind of problem is to compute the *least-squares approximation*, which is the minimizer $\\mathbf{x}$ of the function \n",
    "\n",
    "$$f(\\mathbf{x}) = \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert^2\\; = \\sum_i \\left(b_i - \\sum_j a_{ij} x_{j}\\right)^2.$$\n",
    "\n",
    "Note that, if $\\mathbf{b} \\in \\text{range}(\\mathbf{A})$; that is, if $\\mathbf{b}$ is one of those lucky values such that $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ does indeed have an exact solution, then we can choose $\\mathbf{x}$ such that $f(\\mathbf{x}) = 0$ by finding the exact solution. \n",
    "\n",
    "Otherwise, we need to satisfy ourself with an approximation, i.e. a value $\\mathbf{x}$ such that $f(\\mathbf{x}) > 0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b76991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the solution x, error f(x), rank of A, and singular values of A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645d531",
   "metadata": {},
   "outputs": [],
   "source": [
    " # approximate solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0e02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "131f5156",
   "metadata": {},
   "source": [
    "## Application: Linear Regression, Several Ways\n",
    "\n",
    "Actually, the problem of minimizing $f(\\mathbf{x}) = \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert^2$ has a special name in statistics -- it's linear regression! Specifically, it's *orderinary least-squares multvariate linear regression*.  It's usually written like this: \n",
    "\n",
    "$$f(\\beta) = \\lVert \\mathbf{X}\\beta - \\mathbf{y} \\rVert^2\\;,$$\n",
    "\n",
    "where $\\mathbf{X}$ is the matrix of observations of the dependent variables, and $\\mathbf{y}$ is the vector of observations of the dependent variable. $\\beta$ is the vector of coefficients, and it's the thing that we want to estimate. We do this by finding an estimate $\\hat{\\beta}$ that makes $f(\\hat{\\beta})$ small. \n",
    "\n",
    "By the way, if you are familiar with the topic of *loss functions* in machine learning, this function $f$ is called the *square-error loss* for estimating $\\mathbf{y}$, and is probably the most important of all loss functions for regression tasks. \n",
    "\n",
    "Let's use least-squares approximation to perform 1d linear regression \"by hand\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e44dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formally, x needs to be 2d for this to work\n",
    "# so we give it an extra dimension using reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67701179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79ce780c",
   "metadata": {},
   "source": [
    "This works in any number of dimensions! \n",
    "\n",
    "In fact, this least-squares problem has an analytic solution in terms of matrix inverses as well, given by \n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}\\;.\n",
    "$$\n",
    "\n",
    "This matrix computation is easy to implement in Python using tools from the previous section. Rather than creating a plot, let's show how we perform multidimensional regression and recover an estimate of the coefficient vector $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c8e93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67a66791",
   "metadata": {},
   "source": [
    "The reason that `numpy` implements a specialized least-squares function like `lstsq` is that the analytic approach can be impractical to compute when the number of columns in `X` is large, as matrix inversion is a very slow operation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
