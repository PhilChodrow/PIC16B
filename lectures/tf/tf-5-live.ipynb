{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:PIC16B] *",
      "language": "python",
      "name": "conda-env-PIC16B-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-5-live.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6cdb3f"
      },
      "source": [
        "# Text Generation with Recurrent Neural Networks\n",
        "\n",
        "In this set of lecture notes, we'll consider a new kind of machine learning task. Previously, we've focused on *classification* problems. In classification problems, the goal is to assign a given piece of data to one of several categories. Today, we'll instead consider a simple  *generation* problem. A *generative* model can be used to create \"realistic\" examples after it's been trained. Generative models are at the heart of machine learning topics like [deepfakes](https://en.wikipedia.org/wiki/Deepfake), [language generation](https://aiweirdness.com/post/140219420017/the-silicon-gourmet-training-a-neural-network-to), and [style transfer](https://www.tensorflow.org/tutorials/generative/style_transfer).  \n",
        "\n",
        "*Parts of these lecture notes were based on [this tutorial](https://keras.io/examples/generative/lstm_character_level_text_generation/). It is recommended to run the code contained in these notes in a Google Colab instance with GPU acceleration enabled.* "
      ],
      "id": "4a6cdb3f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b87fcaf5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd"
      ],
      "id": "b87fcaf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "656ab4c1"
      },
      "source": [
        "# link to Google Drive to read in trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "656ab4c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0d456c1"
      },
      "source": [
        "\n",
        "## Our Task\n",
        "\n",
        "Today, we are going to see whether we can teach an algorithm to understand and reproduce the pinnacle of cultural achievement; the benchmark against which all art is to be judged; the mirror that reveals to humany its truest self. I speak, of course, of *Star Trek: Deep Space Nine.*\n",
        "\n",
        "<figure class=\"image\" style=\"width:300px\">\n",
        "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/_images/DS9.jpg\" alt=\"\">\n",
        "  <figcaption><i></i></figcaption>\n",
        "</figure>\n",
        "\n",
        "In particular, we are going to attempt to teach a neural  network to generate *episode scripts*. This a text generation task: after training, our hope is that our model will be able to create scripts that are reasonably realistic in their appearance. \n"
      ],
      "id": "b0d456c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87b9a233"
      },
      "source": [
        "## miscellaneous data cleaning\n",
        "\n",
        "start_episode = 20 # Start in Season 2, Season 1 is not very good\n",
        "num_episodes = 50  # only pick this many episodes to train on\n",
        "\n",
        "url = \"https://github.com/PhilChodrow/PIC16B/blob/master/datasets/star_trek_scripts.json?raw=true\"\n",
        "star_trek_scripts = pd.read_json(url)\n",
        "\n",
        "cleaned = star_trek_scripts[\"DS9\"].str.replace(\"\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts -\", \"\")\n",
        "cleaned = cleaned.str.split(\"\\n\\n\\n\\n\\n\\n\\n\").str.get(-2)\n",
        "text = \"\\n\\n\".join(cleaned[start_episode:(start_episode + num_episodes)])\n",
        "for char in ['\\xa0', 'à', 'é', \"}\", \"{\"]:\n",
        "    text = text.replace(char, \"\")"
      ],
      "id": "87b9a233",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ca5dbc1"
      },
      "source": [
        "The result is one long string containing the scripts of 50 episodes of Star Trek: Deep Space 9. How glorious!"
      ],
      "id": "5ca5dbc1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f913b13"
      },
      "source": [
        ""
      ],
      "id": "3f913b13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f55b17f1"
      },
      "source": [
        ""
      ],
      "id": "f55b17f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b02e991f"
      },
      "source": [
        "Our first step, as usual, is data preparation. What we need to do is format the data in such a way that we can treat the situation as a classification problem after all. That is: \n",
        "\n",
        "> Given a string of text, predict the next character in that string. \n",
        "\n",
        "Doing this repeatedly will allow the model to generate large bodies of text. \n",
        "\n",
        "To do this, we want to split our data like so: \n",
        "\n",
        "```\n",
        "predictor = \"to boldly g\"\n",
        "target    = \"o\"\n",
        "```\n",
        "\n",
        "The following function will do this for us. The `max_len` argument gives the number of characters that should be in the predictor string, and the `step_size` argument lets us skip indices if we want to in order to decrease the size of the data. "
      ],
      "id": "b02e991f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d427d52f"
      },
      "source": [
        ""
      ],
      "id": "d427d52f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "439c4224"
      },
      "source": [
        ""
      ],
      "id": "439c4224",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98290dc9"
      },
      "source": [
        "Our next step is to vectorize the characters. This is similar to the word vectorization task, but it's simple enough in this case that's arguably more convenient to actually handle it outside of TensorFlow. It is also possible to handle vectorization using TensorFlow tools, as demonstrated in [this tutorial](https://www.tensorflow.org/tutorials/text/text_generation). "
      ],
      "id": "98290dc9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4808756b"
      },
      "source": [
        "# vectorization code\n",
        "\n",
        "chars = sorted(set(text))\n",
        "char_indices = {char : chars.index(char) for char in chars}\n",
        "X = np.zeros((len(lines), max_len, len(chars)))\n",
        "y = np.zeros((len(lines), 1), dtype = np.int32)\n",
        "for i, line in enumerate(lines):\n",
        "\tfor t, char in enumerate(line):\n",
        "\t\tX[i, t, char_indices[char]] = 1\n",
        "\ty[i] = char_indices[next_chars[i]]"
      ],
      "id": "4808756b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25635c7c"
      },
      "source": [
        "Let's take a look at the results. "
      ],
      "id": "25635c7c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72802240"
      },
      "source": [
        ""
      ],
      "id": "72802240",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab956cf8"
      },
      "source": [
        ""
      ],
      "id": "ab956cf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b55cdc6"
      },
      "source": [
        "What does this matrix say? `X[0]` is a single sequence of 20 characters. The first row says that the first character of this sequence is the character corresponding to integer `1`. The second row says that the second character corresponds to integer `1`, and so on (`1` is the space character `\" \"`, in case you're wondering).  "
      ],
      "id": "1b55cdc6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c28265e0"
      },
      "source": [
        ""
      ],
      "id": "c28265e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2584fc6"
      },
      "source": [
        "This says that the 21st character (the character after the first 20 characters of the string) is the character with index `42`. "
      ],
      "id": "e2584fc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8266e9d4"
      },
      "source": [
        "Now we're ready to perform a train-test split: "
      ],
      "id": "8266e9d4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb436dbe"
      },
      "source": [
        ""
      ],
      "id": "bb436dbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcc9d669"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Model time! We'll use a simple *Long Short-Term Memory* (LSTM) model for this example. LSTMs are one example of *recurrent* neural network layers. Here's a diagram illustrating the schematic functioning of a recurrent layer. \n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "*Image credit: [Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), OpenAI*\n",
        "\n",
        "On the lefthand side, we have a \"zoomed out\" picture of a recurrent neural network layer. On the righthand side, we see the \"zoomed in\" version. The key point here is that output $h_2$ depends not only on input $x_2$, but also, indirectly, on inputs $x_0$ and $x_1$. This means that recurrent neural networks are highly suitable for modeling processes that have temporal structure. Text is an example: the last few characters are the \"history\" of the text. Timeseries data are another clear example, and indeed, we can use a very similar workflow to the one we'll use today in order to do forecasting in timeseries. \n",
        "\n",
        "Since training for this kind of task gets expensive fast, we'll use just one LSTM layer followed by a `Dense` output layer. "
      ],
      "id": "fcc9d669"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e1f8600"
      },
      "source": [
        ""
      ],
      "id": "5e1f8600",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fe229e3"
      },
      "source": [
        ""
      ],
      "id": "4fe229e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef4dc531"
      },
      "source": [
        "Time for training. We'll do just one epoch for now, mostly just to prove that we've set up our model correctly. "
      ],
      "id": "ef4dc531"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8c3d5f7"
      },
      "source": [
        "# code I used to train and save the model\n",
        "# model.fit(X_train, \n",
        "#           y_train,\n",
        "#           validation_data= (X_val, y_val),\n",
        "#           batch_size=128, epochs = 200)\n",
        "# model.save('/content/drive/MyDrive/DS9_model') \n",
        "\n"
      ],
      "id": "f8c3d5f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51df40ff"
      },
      "source": [
        "Rather than training the entire model live during lecture, I'm instead going to load in a saved model that I previously trained for 200 epochs on Google Colab. On Colab, each epoch takes around 10s or so. 200 epochs corresponds to roughly 30 minutes. "
      ],
      "id": "51df40ff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ca2f6a"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/DS9_model')"
      ],
      "id": "84ca2f6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4801965"
      },
      "source": [
        "Generative models define *probability distributions* over the space of possible outputs. So, our overall algorithm is going to generate new text in a partially randomized way. To make this happen, we define a `sample` function which will take the model outputs, turn them into probabilities, and then sample from the probabilities to produce a single character (well, technically, an integer corresponding to a single character). \n",
        "\n",
        "An important parameter here is the so-called *temperature* (this terminology comes from statistical physics. When the temperature is high, the model will more frequently choose low-probability characters. This is sometimes interpreted as \"creativity,\" and leads to more unpredictable outputs. When the temperature is low, on the other hand, the model will \"play it safe\" and tend to stick to known patterns. In the extreme limiting case as the temperature approaches 0, the model will ultimately get stuck in \"loops\" in which it repeats common phrases over and over again. \n",
        "\n",
        "I'm not going to dwell on the math here, but if you're familiar with phrases like \"softmax\" or \"Boltzmann distribution,\" this code implements and samples from such a distribution using the model predictions."
      ],
      "id": "d4801965"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3d0f50d"
      },
      "source": [
        ""
      ],
      "id": "d3d0f50d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e0f260a"
      },
      "source": [
        "Note that this function takes in some model predictions and returns a single integer, which we can interpret as a character. \n",
        "\n",
        "Now that we know how to sample from the model predictions to create a new character, let's now define a convenient function that will allow us to create entire strings of specified length using this process. There's some index management here that can be a little tricky. "
      ],
      "id": "2e0f260a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "243cb463"
      },
      "source": [
        "def generate_string(seed_index, temp, gen_length, model): \n",
        "    \n",
        "    # sequence of integer indices for generated text\n",
        "    gen_seq = np.zeros((max_len + gen_length, len(chars)))\n",
        "    \n",
        "    # first part of the generated indices actually corresponds to the real text\n",
        "    seed = X[seed_index]\n",
        "    gen_seq[0:max_len] = seed\n",
        "    \n",
        "    # character version\n",
        "    gen_text = lines[seed_index]\n",
        "    \n",
        "    # main loop. \n",
        "    # at each stage we are going to get a single \n",
        "    # character from the model prediction (with the sample function)\n",
        "    # and then feed that character BACK into the model as \"data\"\n",
        "    # for the next prediction\n",
        "    for i in range(0, gen_length):\n",
        "        \n",
        "        # this corresponds to the part of the generated\n",
        "        # text that the model can \"see\"\n",
        "        window = gen_seq[i: i + max_len]\n",
        "        \n",
        "        # get the prediction and sample a single index\n",
        "        preds = model.predict(np.array([window]))[0]\n",
        "        next_index = sample(preds, temp)\n",
        "        \n",
        "        # add sampled index to the current output\n",
        "        gen_seq[max_len + i, next_index] = True\n",
        "        \n",
        "        # create the string version\n",
        "        next_char = chars[next_index]\n",
        "        gen_text += next_char\n",
        "    \n",
        "    # only return the string version because that's what we care about\n",
        "    return(gen_text)"
      ],
      "id": "243cb463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5c7cd47"
      },
      "source": [
        "Let's try it out! We'll create strings of length 500, separating the real seed text from the generated text. We'll also vary the temperature parameter of the `sample()` function, which controls how random the model's predictions can be. "
      ],
      "id": "b5c7cd47"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "gen_length = 1000\n",
        "seed_index = 10002\n",
        "\n",
        "gen = generate_string(seed_index, 0.02, gen_length, model)\n",
        "\n",
        "cast = set(re.findall(r\"[A-Z]+(?=:)\",gen))\n",
        "print(\"CAST OF CHARACTERS: \", end = \"\")\n",
        "print(cast)\n",
        "print(\"-\"*80)\n",
        "print(gen)"
      ],
      "metadata": {
        "id": "D82QACZNK0sj"
      },
      "id": "D82QACZNK0sj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be90aa89"
      },
      "source": [
        "Let's make a few observations. \n",
        "\n",
        "1. First of all, it can take a surprisingly long time to make predictions using our model. This is because we have to call the `predict()` method *for each character*, in order to ensure that the model appropriately takes into account its recent predictions. This can take a pretty long time! \n",
        "2. Second, determining a good value for the temperature can take some experimentation. Note that low temperatures don't necessarily correspond to \"more realistic\" text -- they just correspond to highlighting common patterns in the text, possibly in excess. Higher temperatures also don't necessarily correspond to a \"creative\" algorithm in any normal sense of the word -- set the temperature too high, and you'll just get gibberish. \n",
        "\n",
        "## Specialization\n",
        "\n",
        "In this case, we were able to create a model for generating Star Trek scripts using an instance of Google Colab in roughly 30 minutes. This model is highly limited. Although it clearly has learned some relevant features of Star Trek scripts, there's no way that you'd mistake the output of the model for an actual script by a screenwriter. Considering how hard this was, imagine how much effort and computational resources are required to create more general language models! Indeed, as highlighted in a [recent and controversial paper](https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf), training large language models in this day and age can require energy expenditure comparable to a trans-Atlantic flight! "
      ],
      "id": "be90aa89"
    }
  ]
}