{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dependent-vacuum",
   "metadata": {},
   "source": [
    "# Working with Databases\n",
    "\n",
    "### Takeaways for Today\n",
    "\n",
    "1. Reading large data sets into memory can be surprisingly slow, and in some cases completely impractical. \n",
    "2. Databases offer a convenient way to perform selective *queries* on our data, allowing us to retrieve only the data that we want at any given time. \n",
    "3. The `sqlite3` package enables us to work with databases using Python commands, and the `pandas` package makes it easy to read the results of database queries as data frames. \n",
    "\n",
    "## Intro \n",
    "\n",
    "So far, we've worked exclusively with data that could conveniently fit in memory. Generally speaking, data sets of under 20 megabytes or so can be loaded easily on most modern laptops, although subsequent analysis activities may be slow. Larger data sets can be noticeably slow to even load into memory. For example, you may  have noticed that so far, I've been using a small subset of the NOAA-GHCN data corresponding to the decade 2011-2020. There's much  more data, and I have the complete data set in a file (also uploaded on CCLE) in the same directory as these lecture notes on my personal machine. The complete set of data is not too big to fit in memory, but it's large enough that it's noticeably slow to even load into pandas. Let's take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427f75e-48b0-4ab7-bd4a-b819c2ca83ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5af904-f2cc-4de0-aa3c-30bb3e4f1e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "permanent-voluntary",
   "metadata": {},
   "source": [
    "One second just to load the data set! Note: I'm not even *down*loading the data set -- this is just how long it takes to move the data from my computer's storage into RAM. \n",
    "\n",
    "We can get a pretty good estimate of how much RAM is required to store the data using a handy method of Pandas data frames: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-secret",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-softball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-formation",
   "metadata": {},
   "source": [
    "The data contains 1.35M rows and 14 columns. Altogether, these consume roughly 150 MB of RAM. This is an appreciable fraction of the RAM on most laptops. In some cases, we might need to work with even larger data sets that don't fit in RAM at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-carol",
   "metadata": {},
   "source": [
    "## Introducing Databases\n",
    "\n",
    "When dealing with large data sets like these, it's relatively rare that we absolutely *have* to operate on the entire data set. In most cases, we can work with parts of the data at a time. In the context of the temperature data, \n",
    "\n",
    "1. We might want temperature measurements only between years 1990-2020. \n",
    "2. We might want temperature measurements only for a certain set of countries - maybe in Asia, say. \n",
    "3. We might only want temperature measurements only in the month of March. \n",
    "4. We might want a random 1% of all the data. \n",
    "\n",
    "**Databases** provide us with a structured way to move subsets of data from storage into memory. Python has a module called `sqlite3` (already installed) which we can use to create, manipulate, and query databases. There's also a very handy `pandas` interface, enabling us to efficiently create `pandas` data frames containing exactly the data that we want. Let's get started. \n",
    "\n",
    "### Creating and Populating Databases\n",
    "\n",
    "After importing the `sqlite3` module, the first thing we should do is connect to a database. In case the specified database does not exist, instantiating the connection will also create an empty database with the specified name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-opportunity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-victim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interim-lithuania",
   "metadata": {},
   "source": [
    "A quick check in our file directory reveals that we now have a file called `temps.db`. Great! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-accreditation",
   "metadata": {},
   "source": [
    "There are many ways to add data to a database. Since we are already familiar with Pandas data frames, we'll make use of some extremely convenient functionality which allows us to directly write data frames to a database. \n",
    "\n",
    "But wait! Wasn't the whole point of this to avoid reading in an entire data frame? Indeed! Pandas supplies a nice approach to this using the familiar `pd.read_csv()` function. Supplying a value of `chunksize` will cause `read_csv()` to return not a data frame but an *iterator*, each of whose elements is a piece of the data with number of rows equal to `chunksize`. The data is read \"on the fly\" -- i.e. it's not actually read until we start querying the iterator. Here's an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-shepherd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-memorial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-notebook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "suited-brother",
   "metadata": {},
   "source": [
    "Great! Actually, there are a few cleaning steps that we'll make before incorporating this data into our database. If you're not sure what's going on here, you may want to check the recent lecture in which we cover `stack`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    df = df.set_index(keys=[\"ID\", \"Year\"])\n",
    "    df = df.stack()\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n",
    "    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n",
    "    df[\"Temp\"]  = df[\"Temp\"] / 100\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-prior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "regulation-virginia",
   "metadata": {},
   "source": [
    "Ok! We are finally ready to populate a *table* in our database. \n",
    "\n",
    "You can think of a table as a data frame-like object, represented in SQLite rather than in Python. The `df.to_sql()` method writes to a specified table in the database (the `conn` object from earlier). We need to specify `if_exists` to ensure that we add each piece to the table, rather than overwriting them each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-travel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "informed-weekly",
   "metadata": {},
   "source": [
    "Let's similarly add a table for the metadata in our database. This is a pretty small data set so we don't need to worry about reading it in by chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/noaa-ghcn/station-metadata.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-partner",
   "metadata": {},
   "source": [
    "Now we have a database containing two tables. Let's just check that this is indeed the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "available-ability",
   "metadata": {},
   "source": [
    "Ok wait -- there's actually a lot that just happened here! \n",
    "\n",
    "First: the *cursor* is our primary way to interact with the database. The cursor `execute`s *SQL commands*. SQL is actually its own mini-programming language specifically designed for interacting with databases. We do need to learn a bit of SQL in order to work with databases, but I promise it won't be too bad. \n",
    "\n",
    "Now let's take a look at the actual command that we `execute`d.\n",
    "\n",
    "```sql\n",
    "SELECT name FROM sqlite_master WHERE type='table'\n",
    "```\n",
    "\n",
    "Let's break this down: \n",
    "\n",
    "- `SELECT name`: Show me the entries in the `name` column\n",
    "- `FROM sqlite_master`: of the `sqlite_master` table\n",
    "- `WHERE type='table'`: subject to the condition that the entry in the `type` column of `sqlite_master` is equal to ``table`` (i.e. don't include other kinds of objects)\n",
    "\n",
    "Finally, `cursor.fetchall()` returns the a list containing all the items returned by the query, which we then print.\n",
    "\n",
    "The special `sqlite_master` table contains information on all the objects in the database. We don't usually query it when we want to obtain data, but we can query it to learn about what kinds of tables we have, what their columns are, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-framework",
   "metadata": {},
   "source": [
    "Let's get more detailed information about the items in each table. For example, we can inspect the column names and data types in each. This is a good way to check that we actually populated our database correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-thinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "governing-shelf",
   "metadata": {},
   "source": [
    "This looks pretty good! We have two tables, called `temperatures` and `stations`. The column names are what we would expect. Notice that `sql` has automatically inferred the data types, such as `REAL` and `TEXT`, from the input. Handy! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-anger",
   "metadata": {},
   "source": [
    "#### Basic Queries\n",
    "\n",
    "Now we're ready to perform some basic queries on the data tables themselves. \n",
    "\n",
    "What data do we have for the year 1990? In `pandas` we would do this like: \n",
    "\n",
    "```\n",
    "temperatures[\"id\"][temperatures[\"year\"] == 1990]\n",
    "```\n",
    "\n",
    "The SQL syntax is very different, but contains all the same ideas. \n",
    "\n",
    "- `SELECT`, like the syntax `temperatures[\"id\"]`, controls which column(s) will be returned. \n",
    "- `FROM` tells us which table to return columns from. \n",
    "- `WHERE` is like the Boolean index `[temperatures[\"year\"] == 1990]`. Only rows in which this criterion is satisfied will be returned. \n",
    "\n",
    "**NOTE**: SQL commands are case-insensitive, but it's considered good practice to place SQL keywords in ALL CAPS while names of columns and other content goes in lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-catalyst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-texas",
   "metadata": {},
   "source": [
    "Note that the result is returned as a tuple. This isn't very useful when we are just querying a single column, but we can also get multiple columns, in which case the tuple representation makes a bit more sense: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-basin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "relative-belief",
   "metadata": {},
   "source": [
    "We can include multiple criteria in `WHERE` using the Boolean operators `AND` and `OR`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-reason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "arctic-tuning",
   "metadata": {},
   "source": [
    "Which stations are either far south or far north? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-keyboard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fresh-oriental",
   "metadata": {},
   "source": [
    "## Relations\n",
    "\n",
    "What if we wanted the temperature measurements taken at elevations higher than 1000m above sea-level? This involves information from both the `temperature` table (for the temperatures themselves) and the `stations` table (for elevation info). In order to perform this kind of query, we need to incorporate relational information into our query. This kind of task is very closely related to the `pd.merge` operation that we recently learned. \n",
    "\n",
    "We know from our previous interactions with this data set that the `id` column in the `stations` table corresponds to the `id` column in the `temperature` table. We'd like to incorporate this relationship in our database. From here, the syntax gets only slightly more complicated. Here's what we'll do: \n",
    "\n",
    "1. We'll give the `temperatures` and `stations` tables aliases `T` and `S` just to make the command more readable. We can also split the command onto multiple lines. \n",
    "2. We'll need to add a `LEFT JOIN` command which indicates which column of `T` corresponds to which column of `S`. This is similar to `pd.merge` from a few lectures ago. There are also `RIGHT JOIN`, `INNER JOIN` and `OUTER JOIN` operators that control how duplicates and mismatched rows are handled. The `ON T.id = S.id` portion indicates which columns are expected to correspond. \n",
    "3. As usual, we also need to specify which columns we want (with `SELECT`) and any filter criteria (with `WHERE`). \n",
    "\n",
    "For readability, it's a good idea to break this up into multiple lines. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-powder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-winning",
   "metadata": {},
   "source": [
    "## Tables into Pandas\n",
    "\n",
    "The cursor is very good for prototyping your SQL queries, but once you've gotten the hang of it, the easiest approach is to use another convenience function from Pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "attempted-button",
   "metadata": {},
   "source": [
    "This command takes a long time to execute, and it may seem that we haven't gained much from the database approach. However, it's important to remember that we're not JUST reading a table directly: there's a `JOIN` going on under the hood here. `JOIN`-type operations, including `pd.merge`, can be very expensive computationally. \n",
    "\n",
    "You can actually obtain columns from both tables this way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-enough",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "featured-earthquake",
   "metadata": {},
   "source": [
    "It's good practice to close your database connection once you're done using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-thing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "generous-sculpture",
   "metadata": {},
   "source": [
    "Next time you need to pull some data from your database, just reopen the connection! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-shore",
   "metadata": {},
   "source": [
    "#### *Flashforward -- Sometime in The Future*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-booth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-mailman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "radical-baltimore",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "SQLite3 contains some limited aggregation capabilities. For example, suppose we'd like to compute the average temperature at each station in each month which was either far north or far south. We already know how to do this in Pandas using `df.groupby().aggregate()`. Again, the `SQL` syntax is very different, but contains all the same ideas. The main changes below are that we apply functions to the columns we want to modify, specify a name for the aggregate column, and specify one or more GROUP BY columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-bermuda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-address",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sudden-howard",
   "metadata": {},
   "source": [
    "### Takeaways for Today\n",
    "\n",
    "1. Reading large data sets into memory can be surprisingly slow, and in some cases completely impractical. \n",
    "2. Databases offer a convenient way to perform selective *queries* on our data, allowing us to retrieve only the data that we want at any given time. \n",
    "3. The `sqlite3` package enables us to work with databases using Python commands, and the `pandas` package makes it easy to read the results of database queries as data frames. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
